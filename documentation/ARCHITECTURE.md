# ARCHITECTURE

Table of contents
* [Common information](#common-information)
* [Statistics module](#statistics-module)
* [Airdrop and balances](#airdrop-and-balances)
* [Images uploader](#images-uploader)
* [Entity images](#entity-images)
* users_activity - TODO
* REDIS Redlock - TODO

## Common information


### Libraries
library | description
--- | ---
[ucom.libs.wallet](https://github.com/UOSnetwork/ucom.libs.wallet) | All blockchain-related business logic must be placed to this library.
[ucom.libs.common](https://github.com/UOSnetwork/ucom.libs.common) | Methods, dictionaries, etc. that are used both by frontend and backend applications.
[ucom.libs.social.transactions](https://github.com/UOSnetwork/ucom.libs.social.transactions) | Deprecated library which contains methods to create social transactions.
[ucom.libs.graphql-schemas](https://github.com/UOSnetwork/ucom.libs.graphql-schemas) | A library of methods to make GraphQL requests.
[ucom.web.devops](https://github.com/UOSnetwork/ucom.web.devops) | A repository of DevOps config files related to web application


All blockchain-related business logic must be placed 

### List of main technologies:
* NodeJs
* GraphQL
* REST
* PostgreSQL
* REDIS
* RabbitMq

### Entities list
* Users (Myself as a user state: user + auth token)
* Posts (Publications, reposts, etc.)
* Comments
* Organizations (communities)
* Tags
* Blockchain nodes - for caching blockchain state


### Entities states (response fields structure):
* `Full` - all possible fields
* `Preview` - amount of fields required for lists of entities
* `Card` - minimum amount of fields, used in `object inside object` situations, 
ex. `entity_for` - for whom post is published, User or Organization data.

It is possible that in the future such states will completely be moved to frontend. Because of GraphQL implementation.

## Statistics module

### Goals:
* Event-based model. Save as much as possible raw data.
* Never delete any data in order to be able to make retrospective research and compare the old values of an old algorithm
and the new values of the new calculation algorithm.
* Easy and quick access to the entity current stats values to show on the website pages.


### Solutions

#### Database architecture

There is a table entity_event_param which is used to store `statistics events`. Examples of such events:
* saving of the entity current parameter (current importance, current upvotes amount, etc.)
* calculation of the entity parameter (importance delta for current value and value before 24 hours, etc.)

Notes:
* There are no updates. Possible operations are inserts/selects.
* There is a relational database, same as the main database (PostgreSQL). It is not required yet to implement such solutions
as ClickHouse, because the required period of updating is `1 hour`.
* Statistics events are completely separated from `users activity` - a different kind of events, generated by user.
Statistic event - an event only about some values.
* json_value field contains all stats values, related values, calculation description, the full formula, etc. This is about
the principle `as much information as possible`. In the future, it will be possible to analyze the effectiveness of different
calculation algorithms.
* There are `entity current params` tables, like `posts_current_params`. It has a one-to-one relation with the related entity
(`posts`). This is required to decrease the amount of locks caused by frequent updates.

Scaling strategy:
* This table is placed to separate the database in order to move this database to a separate DB server in the future.
* Partitioning for `hot values` (required for calculation).
* Sharding for `cold values` (required for retrospective).
* Migration to ClickHouse using existing the DB schema.


#### Workers

Currently there are cron-like workers only. In the future, it is possible to use queue-like workers (event-based, not the schedule-based ones).

There are different kinds of workers:
* External data worker (ex. importance worker) - this worker fetches data from external sources (ex. blockchain)
and saves data to the database, at most to `entity_event_params`. (But not always due to the existing legacy solutions)
* Internal data worker - this worker fetches data from internal sources (main relational database) and saves it
to `entity_event_param`. Example - fetch current upvotes and save it.
* Worker-calculator - it uses only `entity_event_param` data to calculate new values. Example - calculate importance
delta.
* Worker-saver - it is used to save current entity stats values to concrete entity tables. For example, update (here update is appeared)
current importance delta for the concrete post. In order to use this value to select posts, the lists are ordered by importance delta
(frontend application is a client for such request). Sometimes it is better to save such values by `worker-calculator`,
because it has all the required values.


#### Code implementation

**Goal**: it should be very easy to add new parameters to stats.

**Main codebase solution:**
* The required parameter and related parameters are declared with the help of special objects with the appropriate Typescript interfaces.
Here is an [example for deltas calculations for posts](../lib/stats/job-params/posts-job-params.ts)
* There are different classes-jobs to save current parameters ([example for posts](../lib/stats/job/posts-stats-job.ts)).
In the future, these classes will be refactored to split declaration and implementation.

**Autotests codebase solution:**
* The before-after-delta parameters sets are generated randomly for every tested parameter. [Reference to generator](../test/generators/entity/entity-event-param-generator-v2.ts)

* [StatsHelper](../test/integration/helpers/stats-helper.ts) has a collection of an expected field structure separated from the main codebase - this is by design. It is required
to code the expected values before implementing the solution (TDD principle). If somebody occasionally changes the implementation, the autotest will fail.
* There is a universal `StatsHelper` parameters checker, based on `expected set` declared manually inside autotests.
* Here is [an example of test suite for posts](../test/integration/stats/stats-only-orgs.test.ts) 

## Airdrop and balances

There are different accounts in the system to represent the balance flows:

type | description
--- | ---
`debt` | To track incoming balances, positive amount
`income` | To track incoming balances, always negative. Required to track new amounts of tokens for an airdrop (`debt` + `income` = 0) 
`reserved` | Amount of tokens user should receive in the future if everything goes ok
`waiting` | Amount of tokens represented by blockchain transaction which is already sent to blockchain
`wallet` | Current amount of airdrop tokens under user's control 


Notes:
* For every new `symbol` new balance account record should be created


### Workflow (with example numbers):

Beforehand - `debt` and `income` accounts must be created.

* There is new token distribution for airdrop, amount = 100 000 tokens (`TNS`). 
* New transaction - from `income` (- 100 000) to `debt` (100 000).
* User is asking about his airdrop state.
* User fulfills some business conditions in order to participate (ex. follows devExchange community)
* Background worker checks conditions and determines users that fulfill all conditions.
* `reserved`, `waiting` and `wallet` accounts are created.
* Transaction is created: from `debt` to `reserved`, amount = 100 TNS
* Other worker fetches `pending` users and creates blockchain transaction (trx).
* Trx is created and is sent to blockchain. Trx ID is received.
?think - what if there backend is down and no following record? - answer: check airdrop state in background
* DB transaction is created: from `reserved` to `waiting`.
* Background worker checks user's airdrop conditions.
** If success - from `waiting` to `wallet`
** If fail - rollback from `waiting` to `reserved`


### Database structure

Please observe the migrations. 
The first one is [create-tables-airdrops-and-accounts](../migrations_knex_monolith/20190320083713_create-tables-airdrops-and-accounts.js)

## Images uploader

Notes:
* This is a separate server to upload and distribute the images.
* Uploader knows nothing about when an image is used. It is only a file storage.
* In order to change an existing image please upload the new one and change the existing link
inside the `entity_images` field.


Workflow:
* Client uploads an image to the uploader application.
* Uploader returns an absolute link to the image. Actually it responds with a JSON structure with the link inside it.
It allows to upload several images in one request without changing the interface.
* Client saves this link in a special JSON structure named `entity_images`. This is actually a column
in a database of table `posts`, `comments`, etc.
* Client sends a post request with `entity_images` to any main application route which supports it.
* The backend application validates the basic `entity_images` structure and saves it inside the database `as-is`.
* In the future, GET request backend application responds to the client and provides a saved `entity_images`
structure.

Benefits:
* Easy to extend. A client application can extend the `entity_images` JSON structure totally by itself without
any backend application changes.
* Easy to scale. It is possible to implement CDN in the future with the same domain.
* Minimum backend development involvement - it is required to add an `entity_images` column to the table
and allow saving of this field.

Further improvements:
* A background task to remove unused images
* Rate limiters per one user per hour/day/month in order to avoid DOS attacks

### An example of possible uploader request-response with a resizing feature (Draft)

Request body
```
sizes: [ // supported only for a .jpg extension
    '800x800',
    '400x400',
    '150x150',
    {
        max_width: 800,
        max_height: 800,
    },
    {


    },
],
// and attach file required to be uploaded
```
Response
content-type: JSON

example for .jpg
```
{
    files: [
      {
        url: `${upload_url}/${filename}.jpg`, // there is no size in the filename!
        type: 'original', // reserved for the future
        width: 600,
        height: 400,
      },
      {
        url: `${upload_url}/${filename}.jpg`,
        type: 'resize',


        size: '400x200', // required only if type is a 'resize'
      },
      {
        url: `${upload_url}/${filename}.jpg`,
        type: 'resize',
        width: 600,
        height: 400,

        resize_id: '800x800',


        size: '600x400', // required only if type is a 'resize'
      },
      {
        url: `${upload_url}/${filename}.jpg`,
        type: 'resize',
        size: '150x50',
      },
    ],
    metadata: { // For future usage
        resize_results: {
            '150x150': {
                status: 'ok',
                requested: '150x150',
                actual: '150x50',
            },
            '400x400': {
                status: 'ok',
                requested: '400x400',
                actual: '400x200',
            },
            '800x800': {
                status: 'impossible',
                requested: '800x800',
                actual: '600x400'
                message: 'Original image size is less than the required value',
            },
        }
    }
};
```

example for .gif
```
{
    files: [
      {
        url: `${upload_url}/${filename}.gif`,
        type: 'original',
      },
    ],
    metadata: {}
};
```

## Entity images

In order to add image(s) to any entity please consider to use a `entity_images` architectural feature.

**The steps to do this:**
* Create a new column using a migration - [example](../migrations_knex_monolith/20190405081637-alter-comments-add-entity-images.js).
* *Optional:* add a new column to an appropriate ORM model.
* Extend an existing model interface by `ModelWithEntityImages`. The interface is [here](../lib/entity-images/interfaces/model-interfaces.ts).
* Make it possible to add a `entity_images` field to model during a creation process via [entity images input service](../lib/entity-images/service/entity-image-input-service.ts).
* Make it possible to update a `entity_images` field (not yet implemented, TODO).
* Write the autotests - [example](../test/integration/comments/comments-entity-images.test.ts).
* Add the `entity_images` field to the GraphQL client library and to the GraphQL backend application server.
* If there are any legacy fields to hold images then please don't forget to migrate existing images from these fields to the new field.

**Related code conventions:**
* Any other existing methods to store entity-related images are deprecated. Any new methods to store images are forbidden.
* Do not use a column name inside the code as a plain string. Use this method:
```
EntityImagesModelProvider.entityImagesColumn()
```
* Keep in mind that a concrete `entity_images` JSON structure is up to the client application.
It is not required to validate it somehow.
* In order to clear a existing `entity_images` value please send empty object `{}`.

**Entity images structure example:**
```
{
   "article_title": [
      {
         "url":"https://backend.u.community/upload/main_image_filename-1545901400947.jpg",
         "type": "original",
      },
      {
         "url":"https://backend.u.community/upload/main_image_filename-123dadsada.jpg",
         "type": "resize",
         "size": '800x800',
      },
   ],
   "main_slider_gallery": [
      'slider_1': {
         "url":"https://backend.u.community/upload/main_image_filename-1545901400947.jpg",
         "type": "original",
         "position": 1,
      },
      {
         "url":"https://backend.u.community/upload/main_image_filename-1545901400947.jpg"
         "type": "original"
         "position": 1,
      },
   ],
}
```
